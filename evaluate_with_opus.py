#!/usr/bin/env python3
"""
Evaluate RepoSpeak outputs using Claude Opus as a judge

This script:
1. Loads a context.json file (generated by Sonnet)
2. Uses Claude Opus to evaluate each component
3. Generates detailed quality ratings and reports
"""

import json
import anthropic
import os
from pathlib import Path
from typing import Dict, List
import config
import time


class OpusEvaluator:
    """Uses Claude Opus to evaluate Sonnet-generated outputs"""

    def __init__(self):
        self.client = anthropic.Anthropic(api_key=config.ANTHROPIC_API_KEY)
        # Use Opus for evaluation
        self.judge_model = "claude-opus-4-5-20251101"

    def _call_api_with_retry(self, prompt: str, max_tokens: int = 2000, max_retries: int = 5) -> str:
        """Call Claude API with exponential backoff retry logic"""
        for attempt in range(max_retries):
            try:
                response = self.client.messages.create(
                    model=self.judge_model,
                    max_tokens=max_tokens,
                    temperature=0,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text.strip()
            except anthropic.OverloadedError as e:
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) + 1  # Exponential backoff: 1, 3, 5, 9, 17 seconds
                    print(f"   ⚠️  API overloaded (attempt {attempt + 1}/{max_retries}). Waiting {wait_time}s")
                    time.sleep(wait_time)
                else:
                    print(f"   ❌ API still overloaded after {max_retries} attempts")
                    raise
            except Exception as e:
                print(f"   ❌ Unexpected error: {e}")
                raise

    def _safe_json_parse(self, response_text: str, default_response: Dict = None) -> Dict:
        """Safely parse JSON response with error handling and auto-fixing"""
        # Remove markdown code blocks if present
        if response_text.startswith("```json"):
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif response_text.startswith("```"):
            response_text = response_text.split("```")[1].strip()

        try:
            return json.loads(response_text)
        except json.JSONDecodeError as e:
            print(f"   ⚠️  JSON parsing error: {e}")
            print(f"   Attempting to fix invalid escape sequences")

       
            import re

            fixed_text = re.sub(r'\\([^"\\/bfnrtu])', r'\\\\\1', response_text)

            try:
                print(f"    Fixed! Parsing corrected JSON")
                return json.loads(fixed_text)
            except json.JSONDecodeError as e2:
                print(f"   ❌ Still failed after fix attempt: {e2}")
                print(f"   Response (first 1000 chars): {response_text[:1000]}")

               
                if default_response:
                    print(f"   ⚠️  Using default response")
                    return default_response

                raise  

    def evaluate_function_summary(self, function_code: str, summary: Dict) -> Dict:
        """Evaluate quality of a function summary"""

        # Handle large functions with OVERLAPPING chunking (same as Sonnet's approach)
        MAX_CODE_LENGTH = 8000 
        code_lines = function_code.split('\n')

        if len(function_code) > MAX_CODE_LENGTH:
            # Chunk large functions with overlap (same strategy as Sonnet)
            CHUNK_LINES = 60  # Lines per chunk
            CHUNK_OVERLAP = 15  # Overlap between chunks for context continuity

            chunks = []
            i = 0
            while i < len(code_lines):
                chunk_end = min(i + CHUNK_LINES, len(code_lines))
                chunk_code = '\n'.join(code_lines[i:chunk_end])
                chunks.append({
                    'code': chunk_code,
                    'start_line': i + 1,
                    'end_line': chunk_end
                })
                # Move forward by (CHUNK_LINES - OVERLAP) for overlapping
                i += (CHUNK_LINES - CHUNK_OVERLAP)
                
                if chunk_end >= len(code_lines):
                    break

            # Format chunked code with overlap indication
            code_display = ""
            for idx, chunk in enumerate(chunks, 1):
                code_display += f"\n--- CHUNK {idx}/{len(chunks)} (lines {chunk['start_line']}-{chunk['end_line']}, overlapping) ---\n{chunk['code']}\n"
        else:
            code_display = function_code

        prompt = f"""You are evaluating the quality of code summaries generated by an AI system.

CODE:
```python
{code_display}
```

GENERATED SUMMARY (by Claude Sonnet):
Human: {summary.get('human', 'N/A')}
Technical: {summary.get('technical', 'N/A')}

Evaluate this summary on the following criteria (rate 1-5 for each):

1. FACTUAL ACCURACY: Does the summary correctly describe what the code does?
   - Are function behaviors correct?
   - Are parameters described accurately?
   - Is the logic flow correct?
   - Are there any hallucinations or factual errors?

2. COMPLETENESS: Does it cover all important aspects?
   - Main functionality covered?
   - Return values described?
   - Side effects noted (if any)?

3. CLARITY: Is it understandable to the target audience?
   - Clear language?
   - Well-structured?

4. TECHNICAL DEPTH: Is the technical summary appropriately detailed?
   - Implementation details covered?
   - Appropriate level of detail?

Respond in JSON format:
{{
    "factual_accuracy": {{
        "rating": <1-5>,
        "errors_found": ["list of errors"],
        "notes": "explanation"
    }},
    "completeness": {{
        "rating": <1-5>,
        "missing_elements": ["list of missing items"],
        "notes": "explanation"
    }},
    "clarity": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "technical_depth": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "overall_score": <1-5>,
    "recommendation": "Accept as-is|Minor issues|Major issues",
    "suggested_improvements": "text"
}}"""

        response_text = self._call_api_with_retry(prompt, max_tokens=2000)

        # Parse JSON response
        default = {
            "factual_accuracy": {"rating": 3, "errors_found": [], "notes": "Evaluation failed"},
            "completeness": {"rating": 3, "missing_elements": [], "notes": "Evaluation failed"},
            "clarity": {"rating": 3, "notes": "Evaluation failed"},
            "technical_depth": {"rating": 3, "notes": "Evaluation failed"},
            "overall_score": 3,
            "recommendation": "Minor issues",
            "suggested_improvements": "JSON parsing error occurred"
        }
        return self._safe_json_parse(response_text, default)

    def evaluate_audio_transcript(self, repo_summary: str, audio_transcript: str) -> Dict:
        """Evaluate quality of audio narration"""

        word_count = len(audio_transcript.split())

        prompt = f"""You are evaluating an audio narration script for a code repository.

REPOSITORY SUMMARY:
{repo_summary}

AUDIO TRANSCRIPT (generated by Claude Sonnet):
{audio_transcript}

The goal is to create an engaging, accessible audio explanation for developers.

Evaluate on these criteria (rate 1-5 for each):

1. ACCURACY: Does it accurately represent the repository?
   - Facts correct?
   - No hallucinations?

2. ANALOGIES: Are analogies present and helpful?
   - Count of analogies found
   - Quality of analogies (relevant? clear?)

3. ACCESSIBILITY: Is it suitable for audio consumption?
   - Acronyms explained?
   - Conversational tone?
   - Avoids jargon?

4. ENGAGEMENT: Does it hook the listener?
   - Opening hook present?
   - Storytelling approach?
   - Appropriate length? (Target: 250-300 words, Actual: {word_count})

5. LISTENABILITY: Would this sound natural when spoken?

Respond in JSON format:
{{
    "accuracy": {{
        "rating": <1-5>,
        "errors_found": ["list"],
        "notes": "explanation"
    }},
    "analogies": {{
        "rating": <1-5>,
        "count": <number>,
        "quality_notes": "explanation"
    }},
    "accessibility": {{
        "rating": <1-5>,
        "acronyms_explained": true/false,
        "notes": "explanation"
    }},
    "engagement": {{
        "rating": <1-5>,
        "has_hook": true/false,
        "word_count": {word_count},
        "notes": "explanation"
    }},
    "listenability": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "overall_score": <1-5>,
    "strengths": ["list"],
    "weaknesses": ["list"]
}}"""

        response_text = self._call_api_with_retry(prompt, max_tokens=3000)
        return self._safe_json_parse(response_text)

    def evaluate_architecture_diagram(self, files: List[str], mermaid_code: str,
                                     logical_groups: Dict) -> Dict:
        """Evaluate architecture diagram quality"""

        files_in_diagram = mermaid_code.count('[') 

        prompt = f"""You are evaluating an architecture diagram generated for a Python repository.

REPOSITORY FILES ({len(files)} total):
{', '.join(files)}

MERMAID DIAGRAM:
```mermaid
{mermaid_code}
```

LOGICAL GROUPS:
{json.dumps(logical_groups, indent=2)}

Evaluate on these criteria (rate 1-5):

1. COMPLETENESS: Are important files included?
   - All major modules present?

2. LOGICAL GROUPING: Are modules grouped sensibly?
   - Groups make semantic sense?
   - Clear separation of concerns?

3. DIAGRAM QUALITY: Is it well-structured?
   - Proper hierarchy?
   - Not too cluttered?

Respond in JSON format:
{{
    "completeness": {{
        "rating": <1-5>,
        "files_in_repo": {len(files)},
        "estimated_files_in_diagram": <number>,
        "notes": "explanation"
    }},
    "logical_grouping": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "diagram_quality": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "overall_score": <1-5>,
    "suggestions": ["list"]
}}"""

        response_text = self._call_api_with_retry(prompt, max_tokens=3000)
        return self._safe_json_parse(response_text)

    def evaluate_repo_summary(self, ast_results: Dict, repo_summary: Dict, module_summaries: Dict) -> Dict:
        """Evaluate repository-level summary"""

        # Get overview of codebase
        total_modules = len([k for k in ast_results.keys() if k != '__analysis_summary__'])
        total_functions = sum(len(m.get('functions', [])) for m in ast_results.values() if isinstance(m, dict))
        total_classes = sum(len(m.get('classes', [])) for m in ast_results.values() if isinstance(m, dict))

        # Get all modules
        module_list = [k for k in ast_results.keys() if k != '__analysis_summary__']

        # Include ALL module summaries for context (full text)
        module_summaries_text = ""
        for module_name, summary in module_summaries.items():
            human_summary = summary.get('human', 'N/A')
            technical_summary = summary.get('technical', 'N/A')
            module_summaries_text += f"\n\n{module_name}:\n  Human: {human_summary}\n  Technical: {technical_summary}"

        prompt = f"""You are evaluating a repository-level summary generated by an AI system.

REPOSITORY STRUCTURE:
- Total Modules: {total_modules}
- Total Functions: {total_functions}
- Total Classes: {total_classes}
- All Modules: {', '.join(module_list)}

MODULE SUMMARIES (by Sonnet - these were used to create the repo summary):
{module_summaries_text}

GENERATED REPOSITORY SUMMARY (by Claude Sonnet):
Human: {repo_summary.get('human', 'N/A')}
Technical: {repo_summary.get('technical', 'N/A')}

Evaluate this repository summary:

1. ACCURACY: Does it correctly describe the repository's purpose and structure?
   - Is the main purpose correctly identified?
   - Are key components mentioned?

2. COMPLETENESS: Does it cover important aspects?
   - Main functionality covered?
   - Key modules/components mentioned?
   - Architecture/structure described?

3. CLARITY: Is it understandable for someone new to the codebase?

4. USEFULNESS: Would this help a developer understand the repo quickly?

Respond in JSON format:
{{
    "accuracy": {{
        "rating": <1-5>,
        "errors_found": ["list"],
        "notes": "explanation"
    }},
    "completeness": {{
        "rating": <1-5>,
        "missing_elements": ["list"],
        "notes": "explanation"
    }},
    "clarity": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "usefulness": {{
        "rating": <1-5>,
        "notes": "explanation"
    }},
    "overall_score": <1-5>,
    "strengths": ["list"],
    "weaknesses": ["list"]
}}"""

        response_text = self._call_api_with_retry(prompt, max_tokens=3000)
        return self._safe_json_parse(response_text)

    def evaluate_module_summaries(self, ast_results: Dict, module_summaries: Dict, function_summaries: Dict) -> Dict:
        """Evaluate ALL module-level summaries"""

        sampled_evaluations = {}

        # Evaluate ALL modules (no sampling)
        for module_name, summary in module_summaries.items():
            print(f"   Evaluating module: {module_name}")

            # Get module code structure from AST
            module_ast = ast_results.get(module_name, {})
            functions = module_ast.get('functions', [])
            classes = module_ast.get('classes', [])
            imports = module_ast.get('imports', [])

            # Get ALL function summaries for this module (full text)
            module_function_summaries = function_summaries.get(module_name, {})
            function_summaries_text = ""
            for func_name, func_summary in module_function_summaries.items():
                if isinstance(func_summary, dict):
                    human_summary = func_summary.get('human', 'N/A')
                    technical_summary = func_summary.get('technical', 'N/A')
                    function_summaries_text += f"\n\n  {func_name}():\n    Human: {human_summary}\n    Technical: {technical_summary}"

            prompt = f"""You are evaluating a module-level summary.

MODULE: {module_name}

MODULE STRUCTURE:
- Functions: {len(functions)} ({', '.join([f['name'] for f in functions]) if functions else 'None'})
- Classes: {len(classes)} ({', '.join([c['name'] for c in classes]) if classes else 'None'})
- Imports: {', '.join([imp.get('module', str(imp)) if isinstance(imp, dict) else str(imp) for imp in imports]) if imports else 'None'}

FUNCTION SUMMARIES (by Sonnet - these were used to create the module summary):
{function_summaries_text if function_summaries_text else "  (No function summaries available)"}

GENERATED MODULE SUMMARY (by Claude Sonnet):
Human: {summary.get('human', 'N/A')}
Technical: {summary.get('technical', 'N/A')}

Evaluate:

1. ACCURACY: Does it correctly describe what this module does?
2. COMPLETENESS: Are key functions/classes mentioned?
3. CLARITY: Is it easy to understand?

Respond in JSON format:
{{
    "accuracy": {{"rating": <1-5>, "notes": ""}},
    "completeness": {{"rating": <1-5>, "notes": ""}},
    "clarity": {{"rating": <1-5>, "notes": ""}},
    "overall_score": <1-5>
}}"""

            response_text = self._call_api_with_retry(prompt, max_tokens=1500)
            sampled_evaluations[module_name] = self._safe_json_parse(response_text)

        # Calculate average
        avg_score = sum(e['overall_score'] for e in sampled_evaluations.values()) / len(sampled_evaluations) if sampled_evaluations else 0

        return {
            "modules_evaluated": list(sampled_evaluations.keys()),
            "individual_evaluations": sampled_evaluations,
            "average_score": round(avg_score, 2)
        }

    def evaluate_dead_code(self, ast_results: Dict, dead_code_analysis: Dict) -> Dict:
        """Evaluate dead code detection (sample validation)"""

        # Evaluate ALL dead code items 
        all_dead_items = []

        # Include ALL dead code categories
        categories = [
            'unreferenced_functions',
            'unused_classes',
            'unused_imports',
            'unused_global_variables',
            'unreachable_code',
            'suspicious_patterns'
        ]

        for category in categories:
            items = dead_code_analysis.get(category, [])
            # Include ALL items - we want to verify Sonnet's classification
            
            for item in items:
                all_dead_items.append((category, item))

        if not all_dead_items:
            return {
                "items_validated": 0,
                "validations": [],
                "summary": {
                    "agreements": 0,
                    "disagreements": 0,
                    "agreement_rate": None  
                },
                "status_breakdown": {
                    "dead_code": {"total": 0, "agreements": 0, "disagreements": 0, "agreement_rate": None},
                    "false_positive": {"total": 0, "agreements": 0, "disagreements": 0, "agreement_rate": None},
                    "uncertain": {"total": 0, "agreements": 0, "disagreements": 0, "agreement_rate": None}
                },
                "category_breakdown": {},
                "note": "No dead code detected - repository is clean! (Not included in overall quality calculation)"
            }

        validations = []

        print(f"   Validating {len(all_dead_items)} dead code items")

        for category, item in all_dead_items:  
            item_name = item.get('name', 'Unknown')
            module = item.get('module', 'Unknown')
            reason = item.get('reason', '')

            # Get full module code context
            module_ast = ast_results.get(module, {})

            # Get Sonnet's classification and evidence
            sonnet_status = item.get('status', 'unknown')
            sonnet_recommendation = item.get('recommendation', 'unknown')
            llm_analysis = item.get('llm_analysis', {})
            sonnet_evidence = llm_analysis.get('evidence', [])

            # Get code snippet if available
            code_snippet = item.get('code', 'Code not available')

            # Build evidence string
            evidence_str = '\n'.join([f"  - {ev}" for ev in sonnet_evidence]) if sonnet_evidence else "  - No evidence provided"

            prompt = f"""You are validating dead code detection accuracy by comparing classifications.

**SONNET'S CLASSIFICATION:**
- Status: {sonnet_status}
- Recommendation: {sonnet_recommendation}
- Category: {llm_analysis.get('category', 'N/A')}
- Confidence: {item.get('confidence', 0)}%
- Sonnet's Reason: {llm_analysis.get('reason', reason)}

**SONNET'S EVIDENCE:**
{evidence_str}

**ITEM DETAILS:**
- Name: {item_name}
- Module: {module}
- Category: {category}
- Line Number: {item.get('lineno', 'N/A')}

**CODE SNIPPET:**
```python
{code_snippet}
```

**MODULE CONTEXT:**
- Functions in module: {', '.join([f['name'] for f in module_ast.get('functions', [])])}
- Classes in module: {', '.join([c['name'] for c in module_ast.get('classes', [])])}

**EVALUATION TASK:**

Review Sonnet's evidence and classification, then make your own independent classification.

**STATUS OPTIONS:**
1. **"dead_code"** - Definitely unused and can be removed
   - Empty/placeholder functions
   - Variables/imports defined but never referenced
   - Code that's clearly never called
   - Evidence shows: "searched codebase, found 1 definition, 0 usages"

2. **"false_positive"** - NOT dead code, should be kept
   - Public API methods (used by external consumers)
   - Protocol/interface implementations (abc, urllib, etc.)
   - Entry points (main, CLI commands, callbacks)
   - Framework hooks (Click decorators, Flask routes, etc.)
   - Library exports (in __init__.py, __all__)

3. **"uncertain"** - Cannot determine with confidence
   - Example/demo code (may or may not be complete)
   - Might be used dynamically (getattr, exec, eval)
   - Conflicting evidence or insufficient context
   - Only use "uncertain" if evidence is genuinely ambiguous

**IMPORTANT:**
- Review Sonnet's EVIDENCE carefully - it contains concrete facts about usage
- If evidence says "Variable defined but no usage found" → likely dead_code
- If evidence says "public API" or "framework hook" → likely false_positive
- Only mark "uncertain" if the evidence itself is unclear or contradictory

**AGREEMENT LOGIC:**
- If your status matches Sonnet's status → verdict: "agree"
- If your status differs from Sonnet's status → verdict: "disagree"

**OUTPUT FORMAT:**
{{
    "opus_status": "dead_code|false_positive|uncertain",
    "verdict": "agree|disagree",
    "confidence": <1-5>,
    "reasoning": "Brief explanation of your classification"
}}

**Examples:**

Example 1 - Agreement (both say dead_code):
- Sonnet status: "dead_code"
- Your status: "dead_code" (empty function, clearly unused)
- Verdict: "agree" ✅

Example 2 - Agreement (both say false_positive):
- Sonnet status: "false_positive"
- Your status: "false_positive" (public API method)
- Verdict: "agree" ✅

Example 3 - Agreement (both say uncertain):
- Sonnet status: "uncertain"
- Your status: "uncertain" (example code, unclear if used)
- Verdict: "agree" ✅

Example 4 - Disagreement (different statuses):
- Sonnet status: "dead_code"
- Your status: "false_positive" (this is a main() entry point)
- Verdict: "disagree" ❌

Example 5 - Disagreement (different statuses):
- Sonnet status: "uncertain"
- Your status: "dead_code" (clearly an unused stub)
- Verdict: "disagree" ❌
"""

            response_text = self._call_api_with_retry(prompt, max_tokens=1000)
            validation = self._safe_json_parse(response_text)
            validation['item_name'] = item_name
            validation['category'] = category
            validation['sonnet_status'] = sonnet_status  # Store Sonnet's classification
            validations.append(validation)

        # Calculate agreement with Sonnet
        agreements = sum(1 for v in validations if v['verdict'] == 'agree')
        disagreements = sum(1 for v in validations if v['verdict'] == 'disagree')

        # Agreement rate = how often Opus agrees with Sonnet's classification
        agreement_rate = round(agreements / len(validations) * 100, 2) if validations else 0

        # Calculate status-level agreement (3-way classification)
        status_breakdown = {
            "dead_code": {"total": 0, "agreements": 0, "disagreements": 0},
            "false_positive": {"total": 0, "agreements": 0, "disagreements": 0},
            "uncertain": {"total": 0, "agreements": 0, "disagreements": 0}
        }

        for v in validations:
            sonnet_status = v.get('sonnet_status', 'unknown')
            if sonnet_status in status_breakdown:
                status_breakdown[sonnet_status]["total"] += 1
                if v['verdict'] == 'agree':
                    status_breakdown[sonnet_status]["agreements"] += 1
                else:
                    status_breakdown[sonnet_status]["disagreements"] += 1

        # Calculate agreement rates per status
        for status, stats in status_breakdown.items():
            if stats["total"] > 0:
                stats["agreement_rate"] = round(stats["agreements"] / stats["total"] * 100, 2)
            else:
                stats["agreement_rate"] = None

        # Also calculate detailed breakdown for ALL categories
        category_breakdown = {}
        all_categories = [
            'unreferenced_functions',
            'unused_classes',
            'unused_imports',
            'unused_global_variables',
            'unreachable_code',
            'suspicious_patterns'
        ]

        for category in all_categories:
            cat_items = [v for v in validations if v['category'] == category]
            if cat_items:
                cat_agreements = sum(1 for v in cat_items if v['verdict'] == 'agree')
                category_breakdown[category] = {
                    "total": len(cat_items),
                    "agreements": cat_agreements,
                    "disagreements": len(cat_items) - cat_agreements,
                    "agreement_rate": round(cat_agreements / len(cat_items) * 100, 2)
                }

        return {
            "items_validated": len(validations),
            "validations": validations,
            "summary": {
                "agreements": agreements,
                "disagreements": disagreements,
                "agreement_rate": agreement_rate
            },
            "status_breakdown": status_breakdown,
            "category_breakdown": category_breakdown,
            "note": f"Opus agrees with Sonnet's classification on {agreements}/{len(validations)} items ({agreement_rate}%)"
        }

    def evaluate_repository(self, context_file: str) -> Dict:
        """Evaluate all components of a repository analysis"""

        print(f"\n{'='*80}")
        print(f" Evaluating Repository Analysis with Claude Opus")
        print(f"{'='*80}")

        # Load context
        with open(context_file, 'r') as f:
            context = json.load(f)

        repo_name = context.get('repo_name', 'Unknown')
        print(f"\n Repository: {repo_name}")

        results = {
            "repo_name": repo_name,
            "evaluator_model": self.judge_model,
            "repo_summary_evaluation": None,
            "module_summaries_evaluation": None,
            "function_summaries": {},
            "audio_evaluation": None,
            "architecture_evaluation": None,
            "dead_code_evaluation": None,
            "overall_metrics": {}
        }

        ast_results = context.get('ast_results', {})

        # 1. Evaluate repository summary
        print(f"\n Evaluating Repository Summary")
        repo_summary = context.get('repo_summary', {})
        module_summaries = context.get('module_summaries', {})
        if repo_summary:
            results['repo_summary_evaluation'] = self.evaluate_repo_summary(
                ast_results, repo_summary, module_summaries
            )

        # 2. Evaluate module summaries
        print(f"\n Evaluating Module Summaries")
        function_summaries = context.get('function_summaries', {})
        if module_summaries:
            results['module_summaries_evaluation'] = self.evaluate_module_summaries(
                ast_results, module_summaries, function_summaries
            )

        # 3. Evaluate ALL function summaries 
        print(f"\n Evaluating Function Summaries")

        all_functions = []
        for module, funcs in function_summaries.items():
            for func_name, summary in funcs.items():
                all_functions.append((module, func_name, summary))

        for module, func_name, summary in all_functions:
            print(f"   Evaluating: {module}::{func_name}")

            # Get function code from AST results
            ast_results = context.get('ast_results', {})
            module_ast = ast_results.get(module, {})
            functions = module_ast.get('functions', [])

            func_code = None
            for func in functions:
                if func.get('name') == func_name:
                    func_code = func.get('code', 'Code not available')
                    break

            if func_code:
                eval_result = self.evaluate_function_summary(func_code, summary)
                results['function_summaries'][f"{module}::{func_name}"] = eval_result

        # 2. Evaluate audio transcript
        print(f"\n Evaluating Audio Transcript")
        audio_narration = context.get('audio_narration', {})
        if audio_narration and 'audio_text' in audio_narration:
            repo_summary = context.get('repo_summary', {}).get('human', '')
            audio_text = audio_narration['audio_text']

            results['audio_evaluation'] = self.evaluate_audio_transcript(
                repo_summary, audio_text
            )

        # 5. Evaluate architecture diagram
        print(f"\n Evaluating Architecture Diagram")
        mermaid_file = Path(f"diagrams/mermaid_{repo_name}.mmd")
        if mermaid_file.exists():
            with open(mermaid_file, 'r') as f:
                mermaid_code = f.read()

            all_files = list(ast_results.keys())
            all_files = [f for f in all_files if f != '__analysis_summary__']
            logical_groups = context.get('logical_groups', {})

            results['architecture_evaluation'] = self.evaluate_architecture_diagram(
                all_files, mermaid_code, logical_groups
            )

        # 6. Evaluate dead code detection
        print(f"\n Evaluating Dead Code Detection")
        dead_code_analysis = context.get('dead_code_analysis', {})
        if dead_code_analysis:
            results['dead_code_evaluation'] = self.evaluate_dead_code(
                ast_results, dead_code_analysis
            )

        # Calculate overall metrics
        print(f"\n Calculating Overall Metrics")
        self._calculate_metrics(results)

        return results

    def _calculate_metrics(self, results: Dict):
        """Calculate aggregate metrics across all evaluations"""

        metrics = {
            "repo_summary_score": 0,
            "module_summary_avg": 0,
            "function_summary_avg": 0,
            "audio_score": 0,
            "architecture_score": 0,
            "dead_code_agreement": 0,
            "overall_quality": 0
        }

        # Repo summary score
        if results['repo_summary_evaluation']:
            metrics['repo_summary_score'] = results['repo_summary_evaluation']['overall_score']

        # Module summaries average
        if results['module_summaries_evaluation']:
            metrics['module_summary_avg'] = results['module_summaries_evaluation']['average_score']

        # Function summaries average
        if results['function_summaries']:
            scores = [eval_data['overall_score']
                     for eval_data in results['function_summaries'].values()]
            metrics['function_summary_avg'] = round(sum(scores) / len(scores), 2) if scores else 0

        # Audio score
        if results['audio_evaluation']:
            metrics['audio_score'] = results['audio_evaluation']['overall_score']

        # Architecture score
        if results['architecture_evaluation']:
            metrics['architecture_score'] = results['architecture_evaluation']['overall_score']

        # Dead code agreement rate (may be None if no dead code detected)
        dead_code_agreement = None
        if results['dead_code_evaluation'] and 'summary' in results['dead_code_evaluation']:
            dead_code_agreement = results['dead_code_evaluation']['summary']['agreement_rate']
            metrics['dead_code_agreement'] = dead_code_agreement if dead_code_agreement is not None else 'N/A'

        # Overall quality (weighted average)
        # If dead code agreement is None (no dead code), redistribute its weight to other components
        if dead_code_agreement is None:
            # No dead code - exclude from calculation, redistribute weight
            total_score = (
                metrics['repo_summary_score'] * (0.15 / 0.85) +      
                metrics['module_summary_avg'] * (0.15 / 0.85) +     
                metrics['function_summary_avg'] * (0.25 / 0.85) +    
                metrics['audio_score'] * (0.15 / 0.85) +             
                metrics['architecture_score'] * (0.15 / 0.85)        
            )
        else:
            # Include dead code in calculation
            total_score = (
                metrics['repo_summary_score'] * 0.15 +
                metrics['module_summary_avg'] * 0.15 +
                metrics['function_summary_avg'] * 0.25 +
                metrics['audio_score'] * 0.15 +
                metrics['architecture_score'] * 0.15 +
                (dead_code_agreement / 100 * 5) * 0.15  
            )
        metrics['overall_quality'] = round(total_score, 2)

        results['overall_metrics'] = metrics


def main():
    """Run evaluation on a context file"""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python3 evaluate_with_opus.py <context_file.json>")
        print("\nExample:")
        print("  python3 evaluate_with_opus.py context_test_project.json")
        sys.exit(1)

    context_file = sys.argv[1]

    if not Path(context_file).exists():
        print(f"❌ Error: Context file not found: {context_file}")
        sys.exit(1)

    # Run evaluation
    evaluator = OpusEvaluator()
    results = evaluator.evaluate_repository(context_file)

    # Save results
    output_file = context_file.replace("context_", "evaluation_")
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)

    # Print summary
    print(f"\n{'='*80}")
    print(f" EVALUATION COMPLETE")
    print(f"{'='*80}")
    print(f"\n Overall Metrics:")
    metrics = results['overall_metrics']
    print(f"   Repo Summary:       {metrics['repo_summary_score']:.2f}/5.0")
    print(f"   Module Summaries:   {metrics['module_summary_avg']:.2f}/5.0")
    print(f"   Function Summaries: {metrics['function_summary_avg']:.2f}/5.0")
    print(f"   Audio Transcript:   {metrics['audio_score']:.2f}/5.0")
    print(f"   Architecture:       {metrics['architecture_score']:.2f}/5.0")

    # Handle dead code agreement 
    if metrics['dead_code_agreement'] == 'N/A':
        print(f"   Dead Code Agreement: N/A (no dead code detected - clean repo!)")
    else:
        print(f"   Dead Code Agreement:{metrics['dead_code_agreement']:.1f}% (Opus agrees with Sonnet)")

    # Show dead code status breakdown if available (3-way classification)
    if results.get('dead_code_evaluation') and 'status_breakdown' in results['dead_code_evaluation']:
        status_breakdown = results['dead_code_evaluation']['status_breakdown']

        print(f"\n    Status Agreement Breakdown (3-way classification):")
        for status, stats in status_breakdown.items():
            if stats['total'] > 0:
                status_display = status.replace('_', ' ').title()
                agreement_pct = stats['agreement_rate'] if stats['agreement_rate'] is not None else 0
                print(f"      {status_display}: {stats['agreements']}/{stats['total']} agreements ({agreement_pct:.0f}%)")

    # Show dead code category breakdown if available
    if results.get('dead_code_evaluation') and 'category_breakdown' in results['dead_code_evaluation']:
        breakdown = results['dead_code_evaluation']['category_breakdown']

        print(f"\n    Category Breakdown:")
        for cat, stats in breakdown.items():
            cat_display = cat.replace('_', ' ').title()
            print(f"      {cat_display}: {stats['agreements']}/{stats['total']} agreements ({stats['agreement_rate']:.0f}%)")

    print(f"\n    Overall Quality:  {metrics['overall_quality']:.2f}/5.0")
    print(f"\n Full results saved to: {output_file}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
